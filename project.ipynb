{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93c4044e",
   "metadata": {},
   "source": [
    "Ok so lets gooo fire emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5f648753",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "label_map = {\n",
    "    \"Normal\": 0,\n",
    "    \"Depression\": 1,\n",
    "    \"Suicidal\": 2,\n",
    "    \"Anxiety\": 3,\n",
    "    \"Stress\": 4,\n",
    "    \"Bi-Polar\": 5,\n",
    "    \"Personality Disorder\": 6\n",
    "}\n",
    "\n",
    "\n",
    "db = pd.read_csv('Sentiment_analysis_dataset(in).csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "85ded796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            Statement      Status\n",
      "0   life doesn’t feel worth it that’s kind of it? ...  Depression\n",
      "1   This life sucks and if it were for my religiou...  Depression\n",
      "2   its been 9 months now for our marriage and she...      Stress\n",
      "3   I do not feel particularly sad or anxious or a...  Depression\n",
      "4   I am taking Venlafaxine. it is an SSRI. does n...  Depression\n",
      "5   I (17M) have come to the realisation that My d...  Depression\n",
      "6   been on fluoxetine for year but anyway there s...  Depression\n",
      "7   even a four day week seems too long i want to ...      Normal\n",
      "8   I often had suicidal thoughts, recently, with ...    Suicidal\n",
      "9   This is the first time facing Pisces. Extro pa...      Normal\n",
      "10  being an orphan at your early stages of life b...  Depression\n",
      "11  The dread of thinking you have cancer... again...     Anxiety\n",
      "12  I (30F) married my husband (31M) about 6 month...      Stress\n",
      "13           What day is Eid Al-Fitr if you may know?      Normal\n",
      "14  Free counseling and support for anyone struggl...     Anxiety\n",
      "15  I am a joke, no friends, everyone thinks I am ...    Suicidal\n",
      "16  Yet he says hes ready to go, nobody wants him ...    Suicidal\n",
      "17  @JonathanRKnight Aww that's really sweet of yo...      Normal\n",
      "19  I have been in a very toxic relationship where...    Suicidal\n",
      "20  If you live alone and no one checks on you, wh...    Suicidal\n"
     ]
    }
   ],
   "source": [
    "db = db.dropna(subset=['Statement', 'Status'])\n",
    "statement = db['Statement'].tolist()\n",
    "status = db['Status'].tolist()\n",
    "\n",
    "print(db.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4f0fc530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data size: 29500\n",
      "Testing data size: 7375\n",
      "Statement    ogbenidipo iwa were gba we see the reason you ...\n",
      "Status                                              Depression\n",
      "Name: 2560, dtype: object\n"
     ]
    }
   ],
   "source": [
    "train_texts, val_texts, train_labels, val_labels = train_test_split(db['Statement'], db['Status'], test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"training data size: {len(train_texts)}\")\n",
    "print(f\"Testing data size: {len(val_texts)}\")\n",
    "print(db.iloc[2542])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8e10e03c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "train_encodings = tokenizer(train_texts.tolist(), truncation=True, padding=True)\n",
    "val_encodings = tokenizer(val_texts.tolist(), truncation=True,padding=True)\n",
    "\n",
    "print(type(train_texts))\n",
    "print(type(train_texts[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "12692a0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36248    i want everyone to feel my pain i am sick of e...\n",
      "26254    Home. Had uni today, got results back :HD, YEH...\n",
      "8331                                          really? why?\n",
      "26739    I am not sure how much longer I can last. I al...\n",
      "18584    I feel so trapped in my mind, the feeling that...\n",
      "Name: Statement, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(train_texts.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95efd8fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of labels: 29500\n",
      "Length of encodings: 2\n",
      "Length of labels: 7375\n",
      "Length of encodings: 2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "class MentalHealthDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "        self.label_map = label_map\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        label = self.labels[idx]\n",
    "        #item[\"labels\"] = torch.tensor(self.label_map[idx])\n",
    "        item[\"labels\"] = torch.tensor(self.label_map[label])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = MentalHealthDataset(train_encodings, train_labels)\n",
    "val_dataset = MentalHealthDataset(val_encodings, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "bba1febc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels = 7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b9ce8e42",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Label 'Depression' not found in label_map.\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[57], line 17\u001b[0m\n\u001b[0;32m      1\u001b[0m training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[0;32m      2\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./results\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      3\u001b[0m     per_device_train_batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      7\u001b[0m     save_strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      8\u001b[0m )\n\u001b[0;32m     10\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m     11\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m     12\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m     13\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mtrain_dataset,\n\u001b[0;32m     14\u001b[0m     eval_dataset\u001b[38;5;241m=\u001b[39mval_dataset,\n\u001b[0;32m     15\u001b[0m )\n\u001b[1;32m---> 17\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Laure\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\trainer.py:2245\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2243\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2244\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2245\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2246\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2250\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Laure\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\trainer.py:2514\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2512\u001b[0m update_step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   2513\u001b[0m num_batches \u001b[38;5;241m=\u001b[39m args\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps \u001b[38;5;28;01mif\u001b[39;00m update_step \u001b[38;5;241m!=\u001b[39m (total_updates \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m remainder\n\u001b[1;32m-> 2514\u001b[0m batch_samples, num_items_in_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_batch_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_batches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2515\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, inputs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(batch_samples):\n\u001b[0;32m   2516\u001b[0m     step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Laure\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\trainer.py:5243\u001b[0m, in \u001b[0;36mTrainer.get_batch_samples\u001b[1;34m(self, epoch_iterator, num_batches, device)\u001b[0m\n\u001b[0;32m   5241\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_batches):\n\u001b[0;32m   5242\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 5243\u001b[0m         batch_samples\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   5244\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m   5245\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Laure\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\accelerate\\data_loader.py:451\u001b[0m, in \u001b[0;36mDataLoaderShard.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    449\u001b[0m \u001b[38;5;66;03m# We iterate one batch ahead to check when we are at the end\u001b[39;00m\n\u001b[0;32m    450\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 451\u001b[0m     current_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m    453\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Laure\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:708\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    707\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 708\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    709\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    712\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    713\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    714\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\Laure\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:764\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    762\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    763\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 764\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    765\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    766\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\Laure\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[55], line 48\u001b[0m, in \u001b[0;36mMentalHealthDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# Ensure that the label exists in the label_map\u001b[39;00m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m label \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel_map:\n\u001b[1;32m---> 48\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLabel \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m not found in label_map.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# Map label to its corresponding integer value\u001b[39;00m\n\u001b[0;32m     51\u001b[0m item[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel_map[label])  \u001b[38;5;66;03m# Map label to integer using label_map\u001b[39;00m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"Label 'Depression' not found in label_map.\""
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
